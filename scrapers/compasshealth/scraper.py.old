import sys, codecs, os
import json
sys.path.append(os.path.dirname(os.path.abspath(__file__)) + '//..//')
import scrapers

#stupid shit because the windows console can't print stuff properly
# sys.stdout = codecs.getwriter('cp850')(sys.stdout.buffer, 'xmlcharrefreplace')
# sys.stderr = codecs.getwriter('cp850')(sys.stderr.buffer, 'xmlcharrefreplace')

def scrape():
	scraper = Scraper('compasshealth')
	
	# Get the list of practices for Wellington
	listUrlSouped = scrapers.openAndSoup('http://www.compasshealth.org.nz/PracticesandFees/WellingtonPractices/PracticeFees.aspx')
	infoURLSouped = scrapers.openAndSoup('http://www.compasshealth.org.nz/PracticesandFees/WellingtonPractices.aspx')
	rows = listUrlSouped.find('table', {'class': 'FeesTable'}).find_all('tr')
	info_table = infoURLSouped.find('table', {'class': 'PracticeTable'})
	info_list = [[cell.get_text(strip=True) or cell.find('img').get('alt') == "No" for cell in row("td")] for row in info_table("tr")[1:]]
	info_dict = {}
	for item in info_list:
		info_dict[item[0]] = item[1:]

	practices_list = []
	error_list = []
	warning_list = []
	current_dir = './compasshealth'

	print("Iterating table...")
	for row in rows:
		cells = row.findAll('td')
		if len(cells) > 0:
			deep = 0
			coord = (0.000, 0.000)
			name = cells[0].find('a').get_text()
			practiceURL = cells[0].find('a').get('href')
			try:
				if info_dict[name][0]:
					error_list.append(practiceURL + " " + name + ": Not enrolling patients.")
					if scrapers.findInDatabase(name):
						scrapers.deletePractice(name)
					continue
			except KeyError:
				print("couldn't find in dict: " + name)
				deep = 1

			print("Found: " + practiceURL)
			try:
				practiceUrlSouped = scrapers.openAndSoup(practiceURL)
			except:
				continue
			if deep:
				addressElement = practiceUrlSouped.find('span', {"id": "dnn_ctr484_Map_AddressLabel"})
				phoneElement = practiceUrlSouped.find('span', {"id": "dnn_ctr484_Map_PhoneLabel"})
				if addressElement is None:
					error_list.append(practiceURL + " " + name +": No address.")
					continue
				else:
					address = addressElement.get_text(strip=True)
					phone = addressElement.get_text(strip=True) if phoneElement else "None supplied"
			else:
				phone = info_dict[name][1]
				address = info_dict[name][2]

			#### GOING IN REALLY DEEP ####
			scriptElement = practiceUrlSouped.find('body').findAll('script', {"type":"text/javascript"})
			first = scriptElement[2].text.split("LatLng(", 1)
			if (len(first) > 1):
				coord = first[1].split(");", 1)[0].split(", ");
				coord[0] = float(coord[0])
				coord[1] = float(coord[1])
			else:
				coord = scrapers.geolocate(address)

			if coord[0] == 0 or coord[1] == 0:
				error_list.append(practiceURL +": Bad coords." + str(coord[0]) + ", " + str(coord[1]))
				continue

			# Make the dictionary object
			practice = {
				'name': name,
				'url': practiceURL,
				'address': address,
				'phone': phone,
				'pho': 'Compass Health',
				'restriction': '',
				'lat': coord[0],
				'lng': coord[1],
				'prices': [
					{
					'age': 0,
					'price': float(cells[1].get_text(strip=True).replace("$", "")),
					},
					{
					'age': 6,
					'price': float(cells[2].get_text(strip=True).replace("$", "")),
					},
					{
					'age': 13,
					'price': float(cells[3].get_text(strip=True).replace("$", "")),
					},
					{
					'age': 18,
					'price': float(cells[4].get_text(strip=True).replace("$", "")),
					},
					{
					'age': 25,
					'price': float(cells[5].get_text(strip=True).replace("$", "")),
					},
					{
					'age': 45,
					'price': float(cells[6].get_text(strip=True).replace("$", "")),
					},
					{
					'age': 65,
					'price': float(cells[7].get_text(strip=True).replace("$", "")),
					},
				] 
			}

			scrapers.postToDatabase(practice, warning_list);
			practices_list.append(practice)

	# Get the list of practices for Wairapa
	listUrlSouped = scrapers.openAndSoup('http://www.compasshealth.org.nz/PracticesandFees/WairarapaPractices/PracticeFees.aspx')
	infoURLSouped = scrapers.openAndSoup('http://www.compasshealth.org.nz/PracticesandFees/WairarapaPractices.aspx')
	rows = listUrlSouped.find('table', {'class': 'FeesTable'}).find_all('tr')
	info_table = infoURLSouped.find('table', {'class': 'PracticeTable'})
	info_list = [[cell.get_text(strip=True) or cell.find('img').get('alt') == "No" for cell in row("td")] for row in info_table("tr")[1:]]
	info_dict = {}
	for item in info_list:
		info_dict[item[0]] = item[1:]
	rows = listUrlSouped.find('table', {'class': 'FeesTable'}).find_all('tr')

	print("Iterating table...")
	for row in rows:
		cells = row.findAll('td')
		if len(cells) > 0:
			deep = 0
			name = cells[0].find('a').get_text()
			practiceURL = cells[0].find('a').get('href')
			coord = (0.000, 0.000)
			try:
				if info_dict[name][0]:
					error_list.append(practiceURL + " " + name + ": Not enrolling patients.")
					if scrapers.findInDatabase(name):
						scrapers.deletePractice(name)
					continue
			except KeyError:
				deep = 1

			practiceUrlSouped = scrapers.openAndSoup(practiceURL)
			if deep:
				addressElement = practiceUrlSouped.find('span', {"id": "dnn_ctr499_Map_AddressLabel"})
				phoneElement = practiceUrlSouped.find('span', {"id": "dnn_ctr499_Map_PhoneLabel"})
				if addressElement is None:
					error_list.append(practiceURL + " " + name +": No address.")
					continue
				else:
					address = addressElement.get_text(strip=True)
					phone = addressElement.get_text(strip=True) if phoneElement else "None supplied"
			else:
				print("Found: " + practiceURL)
				phone = info_dict[name][1]
				address = info_dict[name][2]

			#### GOING IN REALLY DEEP ####
			scriptElement = practiceUrlSouped.find('body').findAll('script', {"type":"text/javascript"})
			first = scriptElement[2].text.split("LatLng(", 1)
			if (len(first) > 1):
				coord = first[1].split(");", 1)[0].split(", ");
				coord[0] = float(coord[0])
				coord[1] = float(coord[1])
			else:
				coord = scrapers.geolocate(address)

			if coord[0] == 0 or coord[1] == 0:
				error_list.append(practiceURL +": Bad coords." + str(coord[0]) + ", " + str(coord[1]))
				continue

			# Make the dictionary object
			practice = {
				'name': name,
				'url': practiceURL,
				'address': address,
				'phone': phone,
				'pho': 'Compass Health',
				'restriction': '',
				'lat': coord[0],
				'lng': coord[1],
				'prices': [
					{
					'age': 0,
					'price': float(cells[1].get_text(strip=True).replace("$", "")),
					},
					{
					'age': 6,
					'price': float(cells[2].get_text(strip=True).replace("$", "")),
					},
					{
					'age': 13,
					'price': float(cells[3].get_text(strip=True).replace("$", "")),
					},
					{
					'age': 18,
					'price': float(cells[4].get_text(strip=True).replace("$", "")),
					},
					{
					'age': 25,
					'price': float(cells[5].get_text(strip=True).replace("$", "")),
					},
					{
					'age': 45,
					'price': float(cells[6].get_text(strip=True).replace("$", "")),
					},
					{
					'age': 65,
					'price': float(cells[7].get_text(strip=True).replace("$", "")),
					},
				] 
			}

			scrapers.postToDatabase(practice, warning_list);
			practices_list.append(practice)


	with open(current_dir + '/data.json', 'w') as outFile:
		json.dump(practices_list, outFile, ensure_ascii=False, sort_keys=True, indent=4)

	scrapers.dealWithFailure(error_list, warning_list, current_dir)